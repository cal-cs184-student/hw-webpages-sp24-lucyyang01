<html>
	<head>
	  <meta charset="UTF-8">
	  <meta name="viewport" content="width=device-width, initial-scale=1.0">
	  <title>Homework 3</title>
	  <style>
	    /* CSS for styling */
	    body {
	      font-family: Arial, sans-serif;
	      line-height: 1.6;
	      margin: 0;
	      padding: 0;
	    }
	    header {
	      background-color: #dd8ea4;
	      color: #fff;
	      padding: 20px;
	      text-align: center;
	    }
	    section {
	      padding: 20px;
	    }
	    h2 {
	      color: #fff; /* Changing text color to contrast with background */
	      text-align: center;
	    }
	    .subsection-content {
	      background-color: #f9f9f9;
	      border: 1px solid #ccc;
	      padding: 10px;
	      border-radius: 5px;
	    }
	    img {
	      max-width: 100%; /* Ensure images don't exceed their container's width */
	      height: auto; /* Maintain aspect ratio */
	      display: block; /* Remove any extra space below images */
	      margin: 0 auto 20px; /* Add margin for spacing */
	      border-radius: 5px; /* Add rounded corners */
	      box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1); /* Add a subtle box shadow */
	    }
	  </style>
	</head>
	<body>
		<header>
		  	<h1>Homework 3: Pathtracer</h1>
		 	<p>By Lucy Yang and Nicole Leung</p>
		</header>

		<section>
			<header>
				<h2>Overview</h2>
				<p>This was a long but rewarding project! In this assignment we rendered reallistic images using a ray-tracer.
					We began with ray generation from a specified point in the world space and defined functions to determine whether or not a generated ray intersects with a primitive
					in the scene. To efficiently divide the mesh we implemented a bounding volume hierarchy. In order to improve the realistic lighting effects of our rasterized images we
					sampled uniformly from a hemisphere and from direct ligt sources. Additionally we introduced global illumination by tracing multiple bounce paths of a ray and 
					included Russian Roulette to randomly terminate ray bounces. Finally we implemented adaptive sampling to dynamically alter the number of samples taken from
				    each pixel.
				</p>
				<p>Throughout the project we encountered a few issues related to debugging.
					Specifically, we had trouble correctly rendering shadows within our image when sampling from direct light sources. We were able to attribute
					these errors to how we were casting our values and computing aritmetic with integers. 
				</p>
				<p>
					Overall, we were able to gain a better conceptual understanding of how ray tracing can render realistic images and the various topics and features used to enhance this process.
				</p>
			</header>
			<div class="subsection-content">

			</div>
		</section>

		<section>
			<header>
				<h2>Part 1: Ray Generation and Scene Intersection</h2>
			</header>
			<div class="subsection-content">
				<p>To generate a ray, we are first given coordinates in world space and convert them to camera space using the following transformations:</p>
					<p>` float sensorX = tan(hFov_rad/ 2) * (2 * x - 1);`</p>
					<p>`float sensorY = tan(vFov_rad/2) * (2 * y - 1);`</p> 
					<p>Hfov and vfov are used to define the horizontal and vertical views of the sensor respectively.</p>
				<p>We then defined the direction of the ray in the camera space originating from the sensor and intersecting our transformed point and transformed this ray back into the world space using the c2w (camera to world) transformation matrix. Finally, we create the ray using the origin of the sensor in world space, and our newly transformed world space direction.
				</p>
				<p>In the triangle intersection algorithm we utilized the Moller Trumblore Algorithm to determine whether or not an intersection occurred. We found the barycentric coordinates of the triangle:</p>
					<p>`b1 = scale * S1.dot(S);` (beta)</p>
					<p>`b2 = scale * S2.dot(r.d);` (gamma)</p>
					<p>(scale = 1/(S1.dot(E1)))</p>
					
					<p>Using two edges of the triangle and their cross-product with the given ray direction and the displacement of the ray-origin from the vertex between the two edges.
					</p>
				<p>We also found the value `t` that scales the direction of the ray:
					<p>`t = scale * S2.dot(E2);`</p>
					<p>If the ray intersects with the triangle we can represent this intersection with our found values above:</p>
					<p>r.o + t * r.d = (1 - b1 - b2)x + b1y + b2z</p>
					</p>
				<p>Therefore, we evaluated whether or not these values were valid (lying in between logical boundaries) in order to determine if an intersection occurred. If the values were valid, implying an intersection, we found the intersected location using the barycentric coordinates specified above. </p>
				<img src="../images/CBempty.png" alt="Description of the image">
				<img src="../images/CBcoil.png" alt="Description of the image">
				<img src="../images/CBspheres.png" alt="Description of the image">
			</div>
		</section>

		<section>
			<header>
				<h2>Part 2: Bounding Volume Hierarchy</h2>
			</header>
			<div class="subsection-content">
				<p>To construct our BVH we first construct a bounding box that encapsulates the entire mesh. Then, we make a new node and check to make sure it has enough primitives in it to make it a leaf or inner node. We find the axis with the largest extent and then sort the iterator over primitives by the centroid of the bounding box of each primitive on along the axis with the largest extent. Then, we compute an index to split the list of primitives in half and recursively call construct_bvh on the left and right nodes. In order to prevent infinite recursion and segfaults when we split the list of primitives in half, we ensure that the distance between the start and end of the iterator is at least 1. </p>
				<p>Without BVH acceleration on cow.dae, we trace 477399 rays with an average speed of 0.0570 rays per second and an average of 1453.732322 intersection tests per ray. The final render took 8.369 seconds to render. With BVH acceleration, we traced 269826 rays with an average speed of 6.0400 million rays per second, averaging 4.064883 intersection tests per ray, for a final rendering time of 0.0447 seconds.</p>
				<img src="../images/cow.png" alt="Description of the image">
				<p>Without BVH acceleration on beetle.dae, we trace 474896 rays with an average speed of 0.0461 million rays per second. We average 1949.850184 intersection tests per ray. The final render took 10.2970 seconds. With BVH acceleration, we traced 230123 rays with an average speed of 9.7144 million rays per second, averaging 2.529069 intersection tests per ray, and the final render took 0.0237 seconds.</p>
				<img src="../images/beetle.png" alt="Description of the image">
			</div>
		</section>

		<section>
			<header>
				<h2>Part 3: Direct Illumination</h2>
			</header>
			<div class="subsection-content">
				<p>In estimate_direct_lighting_hemisphere, we sample uniformly in a hemisphere. This is done by iterating through the total number of samples (determined by the number of lights in the scene) and obtaining a sample from the hemisphereSampler. We then transform this sample into world space, create a new ray and intersection using that sample, and we check for intersection between the ray and the intersection object. If there is an intersection, we multiply the evaluation of the original intersection’s bsdf by the emission of the new intersection’s bsdf along with the cosine term (which is the dot product of our sample and the original intersection’s n field). We multiply this whole term by 2, and divide it by the number of samples outside of the sampling loop.
				</p>
				<p>In estimate_direct_lighting_importance, we sample directly from lights by iterating through all lights in the scene. If it’s a point light, we sample once; otherwise, we sample ns_area_light times for each area light. For each sample loop, we get the emission by calling sample_L on the light. We check if the light is behind the object at the hit point by checking the dot product of the n field of the intersection and the wi vector we obtained from sample_L. We then create a new ray and intersection, and if there is no intersection between the ray and intersection, we multiply the evaluation of the original intersection’s bsdf by our emission along with the cosine term (which is the dot product of our sample and the original intersection’s n field). We divide this by the pdf obtained from sample_L, and average it by the number of samples.</p>
				<p>Here is a bunny rendered with hemisphere sampling:</p>
				<img src="../images/CBbunnyhemisphere.png" alt="Description of the image">
				<p>Here is the same bunny rendered with light sampling.</p>
				<img src="../images/CBbunnyimportance.png" alt="Description of the image">
				<p>In the areas where the bunny has soft shadows, we can observe that the level of noise in these areas decreases significantly as we use more light rays when we use light sampling.</p>
				<p>1 light ray:</p>
				<img src="../images/CBbunnyimportance1ray.png" alt="Description of the image">
				<p>4 light rays:</p>
				<img src="../images/CBbunnyimportance4ray.png" alt="Description of the image">
				<p>16 light rays:</p>
				<img src="../images/CBbunnyimportance16ray.png" alt="Description of the image">
				<p>64 light rays:</p>
				<img src="../images/CBbunnyimportance64ray.png" alt="Description of the image">
				<p>Uniform hemisphere sampling generates a noisier image because directions are sampled uniformly across the hemisphere, which gives us an even distribution of samples. While this is efficient and easy to implement, it leads to noticeable noise in the image, particularly when the lighting is complex. Lighting sampling makes use of importance sampling, which reduces noise in the final image and also leads to faster convergence in areas of the image with strong or complex lighting. However, the tradeoff of a better quality final image is that lighting sampling is more difficult to implement.</p>
			</div>
		</section>

		<section>
			<header>
				<h2>Part 4: Global Illumination</h2>
			</header>
			<div class="subsection-content">
				<p>To implement indirect lighting, we first check to make sure that the maximum ray depth is greater than or equal to 1 - otherwise, we return the one bounce radiance. We accumulate the one bounce radiance and sample the bsdf from the intersection passed in. Then, we obtain the direction vector by transforming the wi vector (populated in the call to sample_f) into world space. We use this direction and the depth of the ray passed in to make a new ray and a new intersection. If there is an intersection between the new ray and new intersection, we recursively call at_least_one_bounce_radiance on this new ray and intersection, find the cosine term by dotting the intersection’s n term with the direction, and then we multiply the recursive return value with the bsdf and cosine and then divide it by the pdf. We accumulate this in the L_out vector. </p>
				<p>To implement Russian Roulette, we set our probability to 0.7 and call coin_flip on this probability. We then divide the accumulated term by the Russian Roulette probability along with the pdf. </p>
				<p>Spheres rendered with global illumination:</p>
				<img src="../images/spheresWRITEUP.png" alt="Description of the image">
				<p>Bunny rendered with global illumination:</p>
				<img src="../images/bunny4rr.png" alt="Description of the image"> 
				<p>isAccumBounces turned off for m=0, 1,2 ,3 4, 5:</p>
					<img src="../images/bunny0rr.png" alt="Description of the image">
					<img src="../images/bunny0rr.png" alt="Description of the image">
					<img src="../images/bunny0rr.png" alt="Description of the image">
					<img src="../images/bunny0rr.png" alt="Description of the image">
					<img src="../images/bunny0rr.png" alt="Description of the image">
				<p>In the second and third bounce of light we can see that the light bouncing off of the spheres from the floor and walls gets dimmer and dimmer. THese bounces of light make the final image look much more convincing because it simulates light reflecting off of the surfaces around the object, which creates more convincing shadows.</p>
				
				
				
				<p>Max ray depths of 0, 1, 2, 3, 4, and 5:</p>
				<img src="../images/bunny0rr.png" alt="Description of the image">
				<img src="../images/bunny1rr.png" alt="Description of the image">
				<img src="../images/bunny2rr.png" alt="Description of the image">
				<img src="../images/bunny3rr.png" alt="Description of the image">
				<img src="../images/bunny4rr.png" alt="Description of the image">
				<img src="../images/bunny5rr.png" alt="Description of the image">

				<p>As max_depth_ray increases, we can see more and more of the color from the walls bleeding into the shadows, as well as the light reflecting off the floor onto the bunny. This gives us more convincing shadows and color information, and allows us to simulate soft shadows and ambient occlusion.
				</p>
				<p>When max_ray_depth = 0 and Russian Roulette, we only have the zero bounce light.
				</p>
				<img src="../images/bunny0rr.png" alt="Description of the image">
				<p>When max_ray_depth = 1 and using Russian Roulette, we can see basic shadow information, but we don’t get any information from bounce lighting off the walls of the color box.
				</p>
				<img src="../images/bunny1rr.png" alt="Description of the image">
				<p>max_depth_ray = 2 and Russian Roulette:</p>
				<img src="../images/bunny2rr.png" alt="Description of the image">
				<p>max_depth_ray = 3 and Russian Roulette:</p>
				<img src="../images/bunny3rr.png" alt="Description of the image">
				<p>max_depth_ray = 4 and Russian Roulette:</p>
				<img src="../images/bunny4rr.png" alt="Description of the image">
				<p>max_depth_ray = 100 and Russian Roulette:</p>
				<img src="../images/bunny100rr.png" alt="Description of the image">
				<p>Comparing sampling rates with 4 light rays:</p>
				<p>Sampling rate = 1:</p>
				<img src="../images/spheresl4s1.png" alt="Description of the image">
				<p>Sampling rate = 2:</p>
				<img src="../images/spheresl4s2.png" alt="Description of the image">
				<p>Sampling rate = 4:</p>
				<img src="../images/spheresl4s4.png" alt="Description of the image">
				<p>Sampling rate = 8:</p>
				<img src="../images/spheresl4s8.png" alt="Description of the image">
				<p>Sampling rate = 16:</p>
				<img src="../images/spheresl4s16.png" alt="Description of the image">
				<p>Sampling rate = 64:</p>
				<img src="../images/spheresl4s64.png" alt="Description of the image">
				<p>Sampling rate = 1024:</p>
				<img src="../images/spheresl4s1024.png" alt="Description of the image">
				<p>Increasing the sampling rate reduces the amount of noise we see in the image, particularly in soft shadows.</p>
			</div>
		</section>

		<section>
			<header>
				<h2>Part 5: Adaptive Sampling</h2>
			</header>
			<div class="subsection-content">
				<p>Essentially, adaptive sampling modifies the number of sample rays taken from each pixel depending on the current sample distribution. It does so by allocating more resources to pixels that require a higher number of samples for accurate rendering and fewer resources to pixels that have already converged. This helps to reduce rendering time and improve the amount of noise in the final image. </p>
				<p>To implement this we can determine if the samples converge by finding the value:
				</p>
				<p>I = 1.96 * standard deviation / sqrt(number of samples)
				</p>
				<p>If I is less that the the maximum tolerance * the mean of the samples, we can conclude that the pixel has converged.</p>
				<p>After generating `samplesPerBatch` number of sample-rays, we calculate the standard deviation, I, and mean of the current sample distribution in order to determine whether or not the samples have converged based on the requirements above. If the samples have converged, we stop generating rays and use the generated sample distribution to return the corresponding radiance.</p>
				<img src="../images/blob_rate.png" alt="Description of the image">
				<img src="../images/bunny_rate.png" alt="Description of the image">
			</div>
		</section>

	
	</body>
</html>
